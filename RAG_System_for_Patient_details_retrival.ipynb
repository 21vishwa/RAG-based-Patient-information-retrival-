{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bo5XownzHQH"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "# Load dataset from the specified path\n",
        "data_path = \"/content/Patient_data.csv\"\n",
        "hospital_data = pd.read_csv(data_path)\n",
        "\n",
        "# Create documents with Anonymous_Uid as the main identifier and all corresponding fields as metadata\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(\n",
        "        page_content=f\"Details for Anonymous_Uid: {doc['Anonymous_Uid']}\",\n",
        "        metadata={k: v if pd.notna(v) else \"NULL\" for k, v in doc.items() if k != \"Anonymous_Uid\"}\n",
        "    )\n",
        "    for doc in tqdm(hospital_data.to_dict(orient=\"records\"))\n",
        "]\n",
        "\n",
        "# Use a custom text splitter to handle tokenization correctly\n",
        "class TokenizerTextSplitter(RecursiveCharacterTextSplitter):\n",
        "    def __init__(self, tokenizer, chunk_size, chunk_overlap, **kwargs):\n",
        "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, **kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def split_text(self, text: str):\n",
        "        print(\"\\n=== Debug: TokenizerTextSplitter ===\")\n",
        "        print(f\"Original text: {text[:10]}...\")  # Show first 200 characters for debugging\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokens = self.tokenizer(\n",
        "            text, truncation=False, add_special_tokens=False, return_offsets_mapping=False\n",
        "        )[\"input_ids\"]\n",
        "        print(f\"Tokenized length: {len(tokens)}\")\n",
        "\n",
        "        # Split tokens into chunks\n",
        "        chunk_size = self._chunk_size\n",
        "        overlap = self._chunk_overlap\n",
        "        chunks = [\n",
        "            tokens[i: i + chunk_size]\n",
        "            for i in range(0, len(tokens), chunk_size - overlap)\n",
        "        ]\n",
        "        print(f\"Number of chunks created: {len(chunks)}\")\n",
        "\n",
        "        # Decode tokens back into readable text chunks\n",
        "        decoded_chunks = [self.tokenizer.decode(chunk, skip_special_tokens=True).strip() for chunk in chunks]\n",
        "        print(f\"Decoded first chunk: {decoded_chunks[0][:10]}...\")  # Show first 200 characters of the first chunk\n",
        "        return decoded_chunks\n",
        "\n",
        "\n",
        "# Split documents into chunks\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str = \"thenlper/gte-small\"\n",
        ") -> List[LangchainDocument]:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    text_splitter = TokenizerTextSplitter(\n",
        "        tokenizer=tokenizer,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Debug: split_documents ===\")\n",
        "    docs_processed = []\n",
        "    for doc_index, doc in enumerate(knowledge_base):\n",
        "        print(f\"\\nProcessing document {doc_index + 1}:\")\n",
        "        print(f\"Original Content: {doc.page_content[:10]}...\")  # Debug the raw content\n",
        "\n",
        "        # Split the document content into chunks\n",
        "        chunks = text_splitter.split_text(doc.page_content)\n",
        "        for chunk_index, chunk in enumerate(chunks):\n",
        "            print(f\"Chunk {chunk_index + 1}: {chunk[:10]}...\")  # Debug each chunk\n",
        "\n",
        "            # Create new LangchainDocument with chunked content\n",
        "            docs_processed.append(\n",
        "                LangchainDocument(page_content=chunk, metadata=doc.metadata)\n",
        "            )\n",
        "\n",
        "    # Ensure uniqueness\n",
        "    unique_docs = {doc.page_content: doc for doc in docs_processed}\n",
        "    print(f\"\\nTotal unique documents processed: {len(unique_docs)}\")\n",
        "    return list(unique_docs.values())\n",
        "\n",
        "\n",
        "docs_processed = split_documents(512, RAW_KNOWLEDGE_BASE)\n",
        "\n",
        "# Visualizing the distribution of document lengths\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()\n",
        "\n",
        "# Create embeddings for documents using HuggingFace Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-small\",\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Create the vector database using FAISS\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
        "\n",
        "# Embed a user query for retrieval\n",
        "user_query = \"Mention all Anonymous_Uid whose Drugname are AUROFLOX EYE DROPS?\"\n",
        "query_vector = embedding_model.embed_query(user_query)\n",
        "\n",
        "# Visualize the embeddings\n",
        "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n",
        "] + [query_vector]\n",
        "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")\n",
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata.get(\"Anonymous_Uid\", \"Unknown\"),\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_layout(title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\")\n",
        "fig.show()\n",
        "\n",
        "# Function to perform RAG (Retrieval Augmented Generation) to answer user queries\n",
        "# Function to perform RAG (Retrieval Augmented Generation) to answer user queries\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    # Retrieve the most relevant documents\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "\n",
        "    # Print the retrieved documents with metadata\n",
        "    print(\"\\n=== Retrieved Relevant Documents ===\")\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i+1}:\")\n",
        "        print(f\"Page Content: {doc.page_content}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Rerank the documents if a reranker is provided\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    # Limit the number of documents to the final number required\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Construct the context from the retrieved documents\n",
        "    context = \"\\nExtracted documents:\\n\" + \"\".join(\n",
        "        [f\"Document {str(i+1)}:\\nContent: {doc.page_content}\\nMetadata: {doc.metadata}\\n\" for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    # Build the final prompt for the LLM\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"Use the context to answer the user's question.Also don't be case sensitive\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n---\\nQuestion: {question}\"},\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    print(\"=> Generating answer...\")\n",
        "    # Generate the answer using the LLM\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "\n",
        "# Example query to get answer from the system\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained model for text generation\n",
        "READER_MODEL_NAME = \"Hugg\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "\n",
        "# Example query to get answer from the system\n",
        "question = \"Mention all Anonymous_Uid whose Drugname are AUROFLOX EYE DROPS?\"\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE)\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import numpy as np\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "# Load dataset from the specified path\n",
        "data_path = \"/content/Patient_data.csv\"\n",
        "hospital_data = pd.read_csv(data_path)\n",
        "hospital_data = hospital_data\n",
        "\n",
        "# Create documents with Anonymous_Uid as the main identifier and all corresponding fields as metadata\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(\n",
        "        page_content=f\"Details for Anonymous_Uid: {doc['Anonymous_Uid']}\",\n",
        "        metadata={k: v if pd.notna(v) else \"NULL\" for k, v in doc.items() if k != \"Anonymous_Uid\"}\n",
        "    )\n",
        "    for doc in tqdm(hospital_data.to_dict(orient=\"records\"))\n",
        "]\n",
        "\n",
        "# Use a custom text splitter to handle tokenization correctly\n",
        "class TokenizerTextSplitter(RecursiveCharacterTextSplitter):\n",
        "    def __init__(self, tokenizer, chunk_size, chunk_overlap, **kwargs):\n",
        "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, **kwargs)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def split_text(self, text: str):\n",
        "        print(\"\\n=== Debug: TokenizerTextSplitter ===\")\n",
        "        print(f\"Original text: {text[:10]}...\")  # Show first 200 characters for debugging\n",
        "\n",
        "        # Tokenize the text\n",
        "        tokens = self.tokenizer(\n",
        "            text, truncation=False, add_special_tokens=False, return_offsets_mapping=False\n",
        "        )[\"input_ids\"]\n",
        "        print(f\"Tokenized length: {len(tokens)}\")\n",
        "\n",
        "        # Split tokens into chunks\n",
        "        chunk_size = self._chunk_size\n",
        "        overlap = self._chunk_overlap\n",
        "        chunks = [\n",
        "            tokens[i: i + chunk_size]\n",
        "            for i in range(0, len(tokens), chunk_size - overlap)\n",
        "        ]\n",
        "        print(f\"Number of chunks created: {len(chunks)}\")\n",
        "\n",
        "        # Decode tokens back into readable text chunks\n",
        "        decoded_chunks = [self.tokenizer.decode(chunk, skip_special_tokens=True).strip() for chunk in chunks]\n",
        "        print(f\"Decoded first chunk: {decoded_chunks[0][:10]}...\")  # Show first 200 characters of the first chunk\n",
        "        return decoded_chunks\n",
        "\n",
        "\n",
        "# Split documents into chunks\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str = \"BAAI/bge-large-en-v1.5\"  # Use new tokenizer model here\n",
        ") -> List[LangchainDocument]:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    text_splitter = TokenizerTextSplitter(\n",
        "        tokenizer=tokenizer,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Debug: split_documents ===\")\n",
        "    docs_processed = []\n",
        "    for doc_index, doc in enumerate(knowledge_base):\n",
        "        print(f\"\\nProcessing document {doc_index + 1}:\")\n",
        "        print(f\"Original Content: {doc.page_content[:10]}...\")  # Debug the raw content\n",
        "\n",
        "        # Split the document content into chunks\n",
        "        chunks = text_splitter.split_text(doc.page_content)\n",
        "        for chunk_index, chunk in enumerate(chunks):\n",
        "            print(f\"Chunk {chunk_index + 1}: {chunk[:10]}...\")  # Debug each chunk\n",
        "\n",
        "            # Create new LangchainDocument with chunked content\n",
        "            docs_processed.append(\n",
        "                LangchainDocument(page_content=chunk, metadata=doc.metadata)\n",
        "            )\n",
        "\n",
        "    # Ensure uniqueness\n",
        "    unique_docs = {doc.page_content: doc for doc in docs_processed}\n",
        "    print(f\"\\nTotal unique documents processed: {len(unique_docs)}\")\n",
        "    return list(unique_docs.values())\n",
        "\n",
        "\n",
        "docs_processed = split_documents(512, RAW_KNOWLEDGE_BASE)\n",
        "\n",
        "# Visualizing the distribution of document lengths\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")  # Use the new tokenizer\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()\n",
        "\n",
        "# Create embeddings for documents using HuggingFace Embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-large-en-v1.5\",  # Use the new embedding model here\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\", \"trust_remote_code\": True},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Create the vector database using FAISS\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
        "\n",
        "# Embed a user query for retrieval\n",
        "user_query = \"Mention all Anonymous_Uid whose drugname is TOBA DROPS ?\"\n",
        "query_vector = embedding_model.embed_query(user_query)\n",
        "\n",
        "# Function to perform RAG (Retrieval Augmented Generation) to answer user queries\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    # Retrieve the most relevant documents\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "\n",
        "    # Print the retrieved documents with metadata\n",
        "    print(\"\\n=== Retrieved Relevant Documents ===\")\n",
        "    for i, doc in enumerate(relevant_docs):\n",
        "        print(f\"Document {i+1}:\")\n",
        "        print(f\"Page Content: {doc.page_content}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Rerank the documents if a reranker is provided\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    # Limit the number of documents to the final number required\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Construct the context from the retrieved documents\n",
        "    context = \"\\nExtracted documents:\\n\" + \"\".join(\n",
        "        [f\"Document {str(i+1)}:\\nContent: {doc.page_content}\\nMetadata: {doc.metadata}\\n\" for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    # Build the final prompt for the LLM\n",
        "    final_prompt = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"Use the context to answer the user's question.Also don't be case sensitive\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n---\\nQuestion: {question}\"},\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    print(\"=> Generating answer...\")\n",
        "    # Generate the answer using the LLM\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "\n",
        "# Load the pre-trained model for text generation\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")\n",
        "\n",
        "# Example query to get answer from the system\n",
        "question = \"Mention all Anonymous_Uid whose drugname is TOBA DROPS ?\"\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE)\n",
        "print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "id": "Ak4iC1QizUdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}